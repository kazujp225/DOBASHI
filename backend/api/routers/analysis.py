"""
Analysis API Router - ã‚³ãƒ¡ãƒ³ãƒˆåé›†ã¨åˆ†æ
"""
from fastapi import APIRouter, HTTPException, BackgroundTasks
from typing import Dict
import json
import os
import time
import sys
from pathlib import Path
from dotenv import load_dotenv

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(os.path.join(os.path.dirname(__file__), "../.."))

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
env_path = Path(__file__).parent.parent.parent.parent / '.env'
if env_path.exists():
    load_dotenv(env_path)
    print(f"[analysis.py] âœ… .env file loaded from: {env_path}")
else:
    print(f"[analysis.py] âš ï¸ .env file not found at: {env_path}")

from collectors.youtube_collector import YouTubeCollector
from analyzers.comment_analyzer import CommentAnalyzer
from aggregators.stats_aggregator import StatsAggregator
from ..schemas import CollectionRequest, CollectionProgress, AnalysisRequest, AnalysisResult, LogEntry

# YouTube API ã‚­ãƒ¼ã‚’ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—
YOUTUBE_API_KEY = os.environ.get('YOUTUBE_API_KEY', '')
if YOUTUBE_API_KEY:
    print(f"[analysis.py] âœ… YOUTUBE_API_KEY loaded: {YOUTUBE_API_KEY[:20]}...")
else:
    print(f"[analysis.py] âš ï¸ YOUTUBE_API_KEY not found in environment")

router = APIRouter()

# é€²æ—ç®¡ç†ç”¨ã®ç°¡æ˜“ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸
collection_status: Dict[str, CollectionProgress] = {}


def add_log(video_id: str, level: str, message: str, emoji: str = None):
    """ãƒ­ã‚°ã‚¨ãƒ³ãƒˆãƒªã‚’è¿½åŠ """
    from datetime import datetime
    if video_id in collection_status:
        log_entry = LogEntry(
            timestamp=datetime.now().isoformat(),
            level=level,
            message=message,
            emoji=emoji
        )
        collection_status[video_id].logs.append(log_entry)


def extract_video_id(url: str) -> str:
    """YouTube URLã‹ã‚‰å‹•ç”»IDã‚’æŠ½å‡º"""
    if "youtube.com/watch?v=" in url:
        return url.split("v=")[1].split("&")[0]
    elif "youtu.be/" in url:
        return url.split("youtu.be/")[1].split("?")[0]
    else:
        return url


@router.post("/collect", response_model=CollectionProgress)
async def collect_comments(request: CollectionRequest, background_tasks: BackgroundTasks):
    """
    YouTubeå‹•ç”»ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’åé›†ï¼ˆãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç†ï¼‰
    """
    video_id = extract_video_id(request.video_url)

    # åˆæœŸã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’è¨­å®š
    collection_status[video_id] = CollectionProgress(
        status="collecting",
        video_id=video_id,
        collected_comments=0,
        message="ã‚³ãƒ¡ãƒ³ãƒˆåé›†ã‚’é–‹å§‹ã—ã¾ã—ãŸ",
        logs=[]
    )

    # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯ã‚’è¿½åŠ 
    background_tasks.add_task(collect_comments_task, video_id)

    return collection_status[video_id]


def collect_comments_task(video_id: str):
    """ã‚³ãƒ¡ãƒ³ãƒˆåé›†ã®ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯"""
    try:
        add_log(video_id, "info", "ğŸš€ ã‚³ãƒ¡ãƒ³ãƒˆåé›†ã‚’é–‹å§‹ã—ã¾ã—ãŸ", "ğŸš€")

        # APIã‚­ãƒ¼ã®ãƒã‚§ãƒƒã‚¯
        if not YOUTUBE_API_KEY:
            add_log(video_id, "error", "âŒ YouTube API ã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“", "âŒ")
            collection_status[video_id] = CollectionProgress(
                status="error",
                video_id=video_id,
                collected_comments=0,
                message="ã‚¨ãƒ©ãƒ¼: YOUTUBE_API_KEYç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“",
                logs=collection_status[video_id].logs
            )
            return

        add_log(video_id, "info", "ğŸ”‘ API ã‚­ãƒ¼ã‚’ç¢ºèªã—ã¾ã—ãŸ", "ğŸ”‘")
        collector = YouTubeCollector(YOUTUBE_API_KEY)

        # å‹•ç”»æƒ…å ±ã‚’å–å¾—
        add_log(video_id, "info", "ğŸ“¹ å‹•ç”»æƒ…å ±ã‚’å–å¾—ä¸­...", "ğŸ“¹")
        video_info = collector.get_video_details(video_id)

        if not video_info:
            add_log(video_id, "error", "âŒ å‹•ç”»æƒ…å ±ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ", "âŒ")
            collection_status[video_id] = CollectionProgress(
                status="error",
                video_id=video_id,
                collected_comments=0,
                message="ã‚¨ãƒ©ãƒ¼: å‹•ç”»æƒ…å ±ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚å‹•ç”»IDãŒæ­£ã—ã„ã‹ç¢ºèªã—ã¦ãã ã•ã„",
                logs=collection_status[video_id].logs
            )
            return

        add_log(video_id, "success", f"âœ… å‹•ç”»æƒ…å ±ã‚’å–å¾—: {video_info['title']}", "âœ…")
        add_log(video_id, "info", f"ğŸ“Š ç·ã‚³ãƒ¡ãƒ³ãƒˆæ•°: {video_info.get('comment_count', 0):,}ä»¶", "ğŸ“Š")

        # ã‚³ãƒ¡ãƒ³ãƒˆã‚’åé›†ï¼ˆå…¨ä»¶å–å¾—ï¼‰
        add_log(video_id, "info", "ğŸ’¬ ã‚³ãƒ¡ãƒ³ãƒˆã‚’åé›†ä¸­...", "ğŸ’¬")
        comments = collector.get_video_comments(video_id, max_results=None)
        add_log(video_id, "success", f"âœ¨ {len(comments):,}ä»¶ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’åé›†ã—ã¾ã—ãŸ", "âœ¨")

        # ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        add_log(video_id, "info", "ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ä¸­...", "ğŸ’¾")
        data_dir = os.path.join(os.path.dirname(__file__), "../../../data")
        os.makedirs(data_dir, exist_ok=True)

        # å‹•ç”»ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        videos_file = os.path.join(data_dir, "videos.json")
        if os.path.exists(videos_file):
            with open(videos_file, 'r', encoding='utf-8') as f:
                videos = json.load(f)
        else:
            videos = []

        # æ—¢å­˜ã®å‹•ç”»ã‚’æ›´æ–°ã¾ãŸã¯è¿½åŠ 
        existing_index = next((i for i, v in enumerate(videos) if v['video_id'] == video_id), None)
        if existing_index is not None:
            videos[existing_index] = video_info
        else:
            videos.append(video_info)

        with open(videos_file, 'w', encoding='utf-8') as f:
            json.dump(videos, f, ensure_ascii=False, indent=2)

        add_log(video_id, "success", "âœ… å‹•ç”»æƒ…å ±ã‚’ä¿å­˜ã—ã¾ã—ãŸ", "âœ…")

        # ã‚³ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        comments_file = os.path.join(data_dir, f"comments_{video_id}.json")
        with open(comments_file, 'w', encoding='utf-8') as f:
            json.dump(comments, f, ensure_ascii=False, indent=2)

        add_log(video_id, "success", "âœ… ã‚³ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ", "âœ…")
        add_log(video_id, "success", "ğŸ‰ ã‚³ãƒ¡ãƒ³ãƒˆåé›†ãŒå®Œäº†ã—ã¾ã—ãŸï¼", "ğŸ‰")

        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’æ›´æ–°
        collection_status[video_id] = CollectionProgress(
            status="completed",
            video_id=video_id,
            collected_comments=len(comments),
            total_comments=video_info.get('comment_count', len(comments)),
            message=f"{len(comments)}ä»¶ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’åé›†ã—ã¾ã—ãŸ",
            logs=collection_status[video_id].logs
        )

    except Exception as e:
        add_log(video_id, "error", f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}", "âŒ")
        collection_status[video_id] = CollectionProgress(
            status="error",
            video_id=video_id,
            collected_comments=0,
            message=f"ã‚¨ãƒ©ãƒ¼: {str(e)}",
            logs=collection_status[video_id].logs if video_id in collection_status else []
        )


@router.get("/collect/{video_id}", response_model=CollectionProgress)
async def get_collection_status(video_id: str):
    """ã‚³ãƒ¡ãƒ³ãƒˆåé›†ã®é€²æ—ã‚’å–å¾—"""
    if video_id not in collection_status:
        raise HTTPException(status_code=404, detail="Collection not found")

    return collection_status[video_id]


@router.post("/analyze", response_model=AnalysisResult)
async def analyze_comments(request: AnalysisRequest):
    """
    åé›†æ¸ˆã¿ã‚³ãƒ¡ãƒ³ãƒˆã‚’åˆ†æ
    """
    start_time = time.time()

    # ã‚³ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    comments_file = os.path.join(
        os.path.dirname(__file__),
        f"../../../data/comments_{request.video_id}.json"
    )

    if not os.path.exists(comments_file):
        raise HTTPException(
            status_code=404,
            detail=f"Comments for video {request.video_id} not found. Please collect first."
        )

    with open(comments_file, 'r', encoding='utf-8') as f:
        comments = json.load(f)

    # ç¤¾é•·ãƒã‚¹ã‚¿ã®ãƒ‘ã‚¹
    tigers_file = os.path.join(os.path.dirname(__file__), "../../../data/tigers.json")
    aliases_file = os.path.join(os.path.dirname(__file__), "../../../data/aliases.json")

    # ç¤¾é•·ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ï¼ˆçµ±è¨ˆç”¨ï¼‰
    with open(tigers_file, 'r', encoding='utf-8') as f:
        all_tigers = json.load(f)

    # æŒ‡å®šã•ã‚ŒãŸç¤¾é•·ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿
    tigers = [t for t in all_tigers if t['tiger_id'] in request.tiger_ids]

    # åˆ†æå®Ÿè¡Œ
    analyzer = CommentAnalyzer(tigers_file, aliases_file)
    analyzed_comments = []

    for comment in comments:
        result = analyzer.find_tiger_mentions(comment['text'], target_tigers=request.tiger_ids)
        analyzed_comments.append({
            **comment,
            'tiger_mentions': result['mentions']  # ç©ºã§ã‚‚OK
        })

    # çµ±è¨ˆé›†è¨ˆ
    aggregator = StatsAggregator(tigers_file)
    stats = aggregator.calculate_video_stats(
        analyzed_comments=analyzed_comments,
        appearing_tigers=request.tiger_ids
    )

    # å‹•ç”»æƒ…å ±ã‚’å–å¾—ã—ã¦titleã‚’è¿½åŠ 
    videos_file = os.path.join(os.path.dirname(__file__), "../../../data/videos.json")
    video_title = "Unknown"
    if os.path.exists(videos_file):
        with open(videos_file, 'r', encoding='utf-8') as f:
            videos = json.load(f)
            video = next((v for v in videos if v['video_id'] == request.video_id), None)
            if video:
                video_title = video.get('title', 'Unknown')

    # çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ï¼ˆãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ç”¨ã«å¤‰æ›ï¼‰
    save_stats = {
        'video_id': request.video_id,
        'title': video_title,
        'total_comments': stats['N_total'],
        'tiger_mention_comments': stats['N_entity'],
        'tiger_stats': [
            {
                'tiger_id': stat['tiger_id'],
                'display_name': stat['display_name'],
                'mention_count': stat['N_tiger'],
                'rate_total': stat['Rate_total'] / 100,  # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆã‚’å°æ•°ã«
                'rate_entity': stat['Rate_entity'] / 100,
                'rank': stat['rank']
            }
            for stat in stats['tiger_stats'].values()
        ]
    }

    stats_file = os.path.join(
        os.path.dirname(__file__),
        f"../../../data/video_stats_{request.video_id}.json"
    )
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump(save_stats, f, ensure_ascii=False, indent=2)

    # åˆ†ææ¸ˆã¿ã‚³ãƒ¡ãƒ³ãƒˆã‚‚ä¿å­˜ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆä¸€è¦§è¡¨ç¤ºç”¨ï¼‰
    analyzed_comments_file = os.path.join(
        os.path.dirname(__file__),
        f"../../../data/analyzed_comments_{request.video_id}.json"
    )
    with open(analyzed_comments_file, 'w', encoding='utf-8') as f:
        json.dump(analyzed_comments, f, ensure_ascii=False, indent=2)

    processing_time = time.time() - start_time

    # è¨€åŠæ•°ã‚’é›†è¨ˆ
    tiger_mentions = {}
    for t in tigers:
        count = sum(
            1 for c in analyzed_comments
            if any(m['tiger_id'] == t['tiger_id'] for m in c['tiger_mentions'])
        )
        tiger_mentions[t['tiger_id']] = count

    return AnalysisResult(
        video_id=request.video_id,
        total_comments=len(comments),
        analyzed_comments=len([c for c in analyzed_comments if c['tiger_mentions']]),
        tiger_mentions=tiger_mentions,
        processing_time=processing_time
    )


@router.get("/comments/{video_id}")
async def get_analyzed_comments(video_id: str, tiger_id: str = None):
    """
    åˆ†ææ¸ˆã¿ã‚³ãƒ¡ãƒ³ãƒˆã‚’å–å¾—ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ç¤¾é•·IDã§ãƒ•ã‚£ãƒ«ã‚¿ï¼‰
    """
    analyzed_comments_file = os.path.join(
        os.path.dirname(__file__),
        f"../../../data/analyzed_comments_{video_id}.json"
    )

    if not os.path.exists(analyzed_comments_file):
        raise HTTPException(
            status_code=404,
            detail=f"Analyzed comments for video {video_id} not found. Please analyze first."
        )

    with open(analyzed_comments_file, 'r', encoding='utf-8') as f:
        analyzed_comments = json.load(f)

    # ç¤¾é•·IDã§ãƒ•ã‚£ãƒ«ã‚¿
    if tiger_id:
        filtered_comments = [
            c for c in analyzed_comments
            if any(m['tiger_id'] == tiger_id for m in c.get('tiger_mentions', []))
        ]
        return filtered_comments

    # è¨€åŠãŒã‚ã‚‹ã‚³ãƒ¡ãƒ³ãƒˆã®ã¿è¿”ã™
    return [c for c in analyzed_comments if c.get('tiger_mentions')]
