"""
Analysis API Router - ã‚³ãƒ¡ãƒ³ãƒˆåé›†ã¨åˆ†æ
"""
from fastapi import APIRouter, HTTPException, BackgroundTasks, Depends
from typing import Dict
import json
import os
import time
import sys
from pathlib import Path
from dotenv import load_dotenv

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(os.path.join(os.path.dirname(__file__), "../.."))

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
env_path = Path(__file__).parent.parent.parent.parent / '.env'
if env_path.exists():
    load_dotenv(env_path)
    print(f"[analysis.py] âœ… .env file loaded from: {env_path}")
else:
    print(f"[analysis.py] âš ï¸ .env file not found at: {env_path}")

from collectors.youtube_collector import YouTubeCollector
from analyzers.comment_analyzer import CommentAnalyzer
from analyzers.tiger_extractor import TigerExtractor
from aggregators.stats_aggregator import StatsAggregator
from ..schemas import CollectionRequest, CollectionProgress, AnalysisRequest, AnalysisResult, LogEntry
from sqlalchemy.orm import Session
from models import get_db, Video as VideoDB, Comment as CommentDB, CommentTigerRelation, VideoTigerStats, VideoTiger, Tiger as TigerDB
from models.database import SessionLocal
from sqlalchemy import delete
from datetime import datetime
import threading

# YouTube API ã‚­ãƒ¼ã‚’ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼ˆå€¤ã¯ãƒ­ã‚°ã«å‡ºã•ãªã„ï¼‰
YOUTUBE_API_KEY = os.environ.get('YOUTUBE_API_KEY', '')
if YOUTUBE_API_KEY:
    print("[analysis.py] âœ… YOUTUBE_API_KEY configured")
else:
    print("[analysis.py] âš ï¸ YOUTUBE_API_KEY not found in environment")

router = APIRouter()

# é€²æ—ç®¡ç†ç”¨ã®ç°¡æ˜“ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ï¼‰
collection_status: Dict[str, CollectionProgress] = {}
collection_locks: Dict[str, threading.Lock] = {}
_status_lock = threading.Lock()  # collection_status/collection_locks ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ç”¨


def get_collection_lock(video_id: str) -> threading.Lock:
    """å‹•ç”»IDã”ã¨ã®ãƒ­ãƒƒã‚¯ã‚’å–å¾—ï¼ˆãªã‘ã‚Œã°ä½œæˆï¼‰"""
    with _status_lock:
        if video_id not in collection_locks:
            collection_locks[video_id] = threading.Lock()
        return collection_locks[video_id]


def add_log(video_id: str, level: str, message: str, emoji: str = None):
    """ãƒ­ã‚°ã‚¨ãƒ³ãƒˆãƒªã‚’è¿½åŠ ï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ï¼‰"""
    from datetime import datetime
    with _status_lock:
        if video_id in collection_status:
            log_entry = LogEntry(
                timestamp=datetime.now().isoformat(),
                level=level,
                message=message,
                emoji=emoji
            )
            collection_status[video_id].logs.append(log_entry)


def extract_video_id(url: str) -> str:
    """YouTube URLã‹ã‚‰å‹•ç”»IDã‚’æŠ½å‡º"""
    if "youtube.com/watch?v=" in url:
        return url.split("v=")[1].split("&")[0]
    elif "youtu.be/" in url:
        return url.split("youtu.be/")[1].split("?")[0]
    else:
        return url


@router.post("/collect", response_model=CollectionProgress)
async def collect_comments(request: CollectionRequest, background_tasks: BackgroundTasks):
    """
    YouTubeå‹•ç”»ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’åé›†ï¼ˆãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç†ï¼‰
    """
    video_id = extract_video_id(request.video_url)

    # åŒã˜å‹•ç”»ã®åŒæ™‚åé›†ã‚’ãƒã‚§ãƒƒã‚¯
    with _status_lock:
        if video_id in collection_status and collection_status[video_id].status == "collecting":
            return collection_status[video_id]  # æ—¢ã«åé›†ä¸­

        # åˆæœŸã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’è¨­å®š
        collection_status[video_id] = CollectionProgress(
            status="collecting",
            video_id=video_id,
            collected_comments=0,
            message="ã‚³ãƒ¡ãƒ³ãƒˆåé›†ã‚’é–‹å§‹ã—ã¾ã—ãŸ",
            logs=[]
        )

    # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯ã‚’è¿½åŠ 
    background_tasks.add_task(collect_comments_task, video_id)

    return collection_status[video_id]


def collect_comments_task(video_id: str):
    """ã‚³ãƒ¡ãƒ³ãƒˆåé›†ã®ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯"""
    try:
        add_log(video_id, "info", "ğŸš€ ã‚³ãƒ¡ãƒ³ãƒˆåé›†ã‚’é–‹å§‹ã—ã¾ã—ãŸ", "ğŸš€")

        # APIã‚­ãƒ¼ã®ãƒã‚§ãƒƒã‚¯
        if not YOUTUBE_API_KEY:
            add_log(video_id, "error", "âŒ YouTube API ã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“", "âŒ")
            collection_status[video_id] = CollectionProgress(
                status="error",
                video_id=video_id,
                collected_comments=0,
                message="ã‚¨ãƒ©ãƒ¼: YOUTUBE_API_KEYç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“",
                logs=collection_status[video_id].logs
            )
            return

        add_log(video_id, "info", "ğŸ”‘ API ã‚­ãƒ¼ã‚’ç¢ºèªã—ã¾ã—ãŸ", "ğŸ”‘")
        collector = YouTubeCollector(YOUTUBE_API_KEY)

        # å‹•ç”»æƒ…å ±ã‚’å–å¾—
        add_log(video_id, "info", "ğŸ“¹ å‹•ç”»æƒ…å ±ã‚’å–å¾—ä¸­...", "ğŸ“¹")
        video_info = collector.get_video_details(video_id)

        if not video_info:
            add_log(video_id, "error", "âŒ å‹•ç”»æƒ…å ±ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ", "âŒ")
            collection_status[video_id] = CollectionProgress(
                status="error",
                video_id=video_id,
                collected_comments=0,
                message="ã‚¨ãƒ©ãƒ¼: å‹•ç”»æƒ…å ±ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚å‹•ç”»IDãŒæ­£ã—ã„ã‹ç¢ºèªã—ã¦ãã ã•ã„",
                logs=collection_status[video_id].logs
            )
            return

        add_log(video_id, "success", f"âœ… å‹•ç”»æƒ…å ±ã‚’å–å¾—: {video_info['title']}", "âœ…")
        estimated_count = video_info.get('comment_count', 0)
        add_log(video_id, "info", f"ğŸ“Š ã‚³ãƒ¡ãƒ³ãƒˆæ•°ï¼ˆæ¨å®šï¼‰: ç´„{estimated_count:,}ä»¶", "ğŸ“Š")

        # ã‚³ãƒ¡ãƒ³ãƒˆã‚’åé›†ï¼ˆå…¨ä»¶å–å¾—ï¼‰
        add_log(video_id, "info", "ğŸ’¬ ã‚³ãƒ¡ãƒ³ãƒˆã‚’åé›†ä¸­...", "ğŸ’¬")
        comments = collector.get_video_comments(video_id, max_results=None)
        add_log(video_id, "success", f"âœ¨ {len(comments):,}ä»¶ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’åé›†å®Œäº†", "âœ¨")

        # ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        add_log(video_id, "info", "ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ä¸­...", "ğŸ’¾")
        data_dir = os.path.join(os.path.dirname(__file__), "../../../data")
        os.makedirs(data_dir, exist_ok=True)

        # å‹•ç”»ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        videos_file = os.path.join(data_dir, "videos.json")
        if os.path.exists(videos_file):
            with open(videos_file, 'r', encoding='utf-8') as f:
                videos = json.load(f)
        else:
            videos = []

        # æ—¢å­˜ã®å‹•ç”»ã‚’æ›´æ–°ã¾ãŸã¯è¿½åŠ 
        existing_index = next((i for i, v in enumerate(videos) if v['video_id'] == video_id), None)
        if existing_index is not None:
            videos[existing_index] = video_info
        else:
            videos.append(video_info)

        with open(videos_file, 'w', encoding='utf-8') as f:
            json.dump(videos, f, ensure_ascii=False, indent=2)

        add_log(video_id, "success", "âœ… å‹•ç”»æƒ…å ±ã‚’ä¿å­˜ã—ã¾ã—ãŸ", "âœ…")

        # ã‚³ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        comments_file = os.path.join(data_dir, f"comments_{video_id}.json")
        with open(comments_file, 'w', encoding='utf-8') as f:
            json.dump(comments, f, ensure_ascii=False, indent=2)

        add_log(video_id, "success", "âœ… ã‚³ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ", "âœ…")

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«å‹•ç”»æƒ…å ±ã‚’ä¿å­˜
        add_log(video_id, "info", "ğŸ—„ï¸ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ä¸­...", "ğŸ—„ï¸")
        db = SessionLocal()
        try:
            # æ—¢å­˜ã®å‹•ç”»ã‚’ç¢ºèª
            existing_video = db.query(VideoDB).filter(VideoDB.video_id == video_id).first()
            if existing_video:
                # æ›´æ–°
                existing_video.title = video_info.get('title', '')
                existing_video.description = video_info.get('description', '')
                existing_video.thumbnail_url = video_info.get('thumbnail_url', '')
                existing_video.view_count = video_info.get('view_count', 0)
                existing_video.like_count = video_info.get('like_count', 0)
                existing_video.comment_count = video_info.get('comment_count', 0)
            else:
                # æ–°è¦ä½œæˆ - published_at ã‚’ datetime ã«å¤‰æ›
                published_at_str = video_info.get('published_at')
                published_at_dt = None
                if published_at_str:
                    try:
                        # ISO 8601 å½¢å¼ã‚’ãƒ‘ãƒ¼ã‚¹
                        published_at_dt = datetime.fromisoformat(published_at_str.replace('Z', '+00:00'))
                    except (ValueError, AttributeError):
                        pass

                new_video = VideoDB(
                    video_id=video_id,
                    title=video_info.get('title', ''),
                    description=video_info.get('description', ''),
                    thumbnail_url=video_info.get('thumbnail_url', ''),
                    published_at=published_at_dt,
                    view_count=video_info.get('view_count', 0),
                    like_count=video_info.get('like_count', 0),
                    comment_count=video_info.get('comment_count', 0)
                )
                db.add(new_video)
            db.commit()
            add_log(video_id, "success", "âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã—ã¾ã—ãŸ", "âœ…")

            # ç¤¾é•·ã‚’è‡ªå‹•æŠ½å‡ºãƒ»ä¿å­˜
            add_log(video_id, "info", "ğŸ” æ¦‚è¦æ¬„ã‹ã‚‰ç¤¾é•·ã‚’è‡ªå‹•æ¤œå‡ºä¸­...", "ğŸ”")
            extractor = TigerExtractor(db)
            result = extractor.extract_tigers(video_id)

            if result.get('success') and result.get('total_tigers_found', 0) > 0:
                tiger_names = [t['display_name'] for t in result.get('tigers', [])]
                add_log(video_id, "success", f"âœ… {len(tiger_names)}åã®ç¤¾é•·ã‚’æ¤œå‡ºãƒ»ç™»éŒ²: {', '.join(tiger_names)}", "âœ…")
            else:
                add_log(video_id, "info", "â„¹ï¸ æ¦‚è¦æ¬„ã‹ã‚‰ç¤¾é•·ã‚’æ¤œå‡ºã§ãã¾ã›ã‚“ã§ã—ãŸï¼ˆåˆ†ææ™‚ã«æ‰‹å‹•é¸æŠå¯èƒ½ï¼‰", "â„¹ï¸")

            # æœªç™»éŒ²ã®åå‰ãŒã‚ã‚Œã°è­¦å‘Š
            unmatched = result.get('unmatched_names', [])
            if unmatched:
                add_log(video_id, "warning", f"âš ï¸ æœªç™»éŒ²ã®ç¤¾é•·åã‚’æ¤œå‡º: {', '.join(unmatched)}ï¼ˆç¤¾é•·ç®¡ç†ã‹ã‚‰ç™»éŒ²ã—ã¦ãã ã•ã„ï¼‰", "âš ï¸")
        except Exception as e:
            add_log(video_id, "warning", f"âš ï¸ ç¤¾é•·è‡ªå‹•æ¤œå‡ºã§ã‚¨ãƒ©ãƒ¼: {str(e)}", "âš ï¸")
        finally:
            db.close()

        add_log(video_id, "success", "ğŸ‰ ã‚³ãƒ¡ãƒ³ãƒˆåé›†ãŒå®Œäº†ã—ã¾ã—ãŸï¼", "ğŸ‰")

        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’æ›´æ–°
        collection_status[video_id] = CollectionProgress(
            status="completed",
            video_id=video_id,
            collected_comments=len(comments),
            total_comments=video_info.get('comment_count', len(comments)),
            message=f"{len(comments)}ä»¶ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’åé›†ã—ã¾ã—ãŸ",
            logs=collection_status[video_id].logs
        )

    except Exception as e:
        add_log(video_id, "error", f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}", "âŒ")
        collection_status[video_id] = CollectionProgress(
            status="error",
            video_id=video_id,
            collected_comments=0,
            message=f"ã‚¨ãƒ©ãƒ¼: {str(e)}",
            logs=collection_status[video_id].logs if video_id in collection_status else []
        )


@router.get("/collect/{video_id}", response_model=CollectionProgress)
async def get_collection_status(video_id: str):
    """ã‚³ãƒ¡ãƒ³ãƒˆåé›†ã®é€²æ—ã‚’å–å¾—"""
    if video_id not in collection_status:
        raise HTTPException(status_code=404, detail="Collection not found")

    return collection_status[video_id]


@router.post("/analyze", response_model=AnalysisResult)
async def analyze_comments(request: AnalysisRequest, db: Session = Depends(get_db)):
    """
    åé›†æ¸ˆã¿ã‚³ãƒ¡ãƒ³ãƒˆã‚’åˆ†æ
    """
    start_time = time.time()

    # ã‚³ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    comments_file = os.path.join(
        os.path.dirname(__file__),
        f"../../../data/comments_{request.video_id}.json"
    )

    if not os.path.exists(comments_file):
        raise HTTPException(
            status_code=404,
            detail=f"Comments for video {request.video_id} not found. Please collect first."
        )

    # JSONãƒ‘ãƒ¼ã‚¹å¤±æ•—ã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
    try:
        with open(comments_file, 'r', encoding='utf-8') as f:
            comments = json.load(f)
    except json.JSONDecodeError as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to parse comments file: {str(e)}"
        )

    # ç¤¾é•·ãƒã‚¹ã‚¿ã®ãƒ‘ã‚¹
    tigers_file = os.path.join(os.path.dirname(__file__), "../../../data/tigers.json")
    aliases_file = os.path.join(os.path.dirname(__file__), "../../../data/aliases.json")

    # ç¤¾é•·ãƒ‡ãƒ¼ã‚¿ãƒ»ã‚¨ã‚¤ãƒªã‚¢ã‚¹èª­ã¿è¾¼ã¿ï¼ˆçµ±è¨ˆãƒ»è¡¨ç¤ºåä»˜ä¸ãƒ»IDè§£æ±ºç”¨ï¼‰
    with open(tigers_file, 'r', encoding='utf-8') as f:
        all_tigers = json.load(f)
    with open(aliases_file, 'r', encoding='utf-8') as f:
        aliases_dict = json.load(f)
    tiger_name_map = {t['tiger_id']: t.get('display_name', t['tiger_id']) for t in all_tigers}

    # å…¥åŠ›IDï¼ˆtigers.jsonã®IDã‹ã‚‚ã—ã‚Œãªã„ï¼‰â†’ ã‚¨ã‚¤ãƒªã‚¢ã‚¹å´IDï¼ˆaliases.jsonã®ã‚­ãƒ¼ï¼‰ã«è§£æ±º
    def resolve_target_ids(input_ids: list[str]) -> tuple[list[str], dict[str, str]]:
        # ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã«å­˜åœ¨ã™ã‚‹ã‚‚ã®ã¯ãã®ã¾ã¾ã€å­˜åœ¨ã—ãªã„å ´åˆã¯display_name/full_nameä¸€è‡´ã§æ¢ç´¢
        alias_ids: list[str] = []
        alias_to_requested: dict[str, str] = {}
        # æ¤œç´¢ç”¨: display_name/full_name ã‚»ãƒƒãƒˆ
        display_map = {t['tiger_id']: (t.get('display_name',''), t.get('full_name','')) for t in all_tigers}
        for req_id in input_ids:
            if req_id in aliases_dict:
                alias_ids.append(req_id)
                alias_to_requested[req_id] = req_id
                continue
            dname, fname = display_map.get(req_id, ('',''))
            matched_key = None
            if dname:
                for k, alias_list in aliases_dict.items():
                    if any(a.get('alias') == dname for a in alias_list):
                        matched_key = k
                        break
            if not matched_key and fname:
                for k, alias_list in aliases_dict.items():
                    if any(a.get('alias') == fname for a in alias_list):
                        matched_key = k
                        break
            # è¦‹ã¤ã‹ã£ãŸå ´åˆã¯aliasã‚­ãƒ¼ã‚’ç”¨ã„ã‚‹ã€ãªã‘ã‚Œã°å…ƒIDã‚’ä½¿ç”¨ï¼ˆæ¤œå‡ºã¯æœŸå¾…è–„ï¼‰
            alias_key = matched_key or req_id
            alias_ids.append(alias_key)
            alias_to_requested[alias_key] = req_id
        # é‡è¤‡é™¤å»ã‚’ä¿ã¡ã¤ã¤é †åºç¶­æŒ
        seen = set()
        alias_ids_unique = []
        for a in alias_ids:
            if a not in seen:
                seen.add(a)
                alias_ids_unique.append(a)
        return alias_ids_unique, alias_to_requested

    # æŒ‡å®šã•ã‚ŒãŸç¤¾é•·ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿
    tigers = [t for t in all_tigers if t['tiger_id'] in request.tiger_ids]

    # åˆ†æå®Ÿè¡Œ
    analyzer = CommentAnalyzer(tigers_file, aliases_file)
    # IDè§£æ±º
    resolved_ids, alias_to_requested = resolve_target_ids(request.tiger_ids)
    analyzed_comments = []

    for comment in comments:
        result = analyzer.find_tiger_mentions(comment.get('text', ''), target_tigers=resolved_ids)

        # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰æœŸå¾…å½¢å¼ã«æ•´å½¢
        mentions_for_ui = [
            {
                # analyzerã®IDï¼ˆã‚¨ã‚¤ãƒªã‚¢ã‚¹å´ï¼‰â†’ ãƒªã‚¯ã‚¨ã‚¹ãƒˆIDï¼ˆtigers.jsonå´ï¼‰ã«æˆ»ã™
                'tiger_id': alias_to_requested.get(m['tiger_id'], m['tiger_id']),
                'display_name': tiger_name_map.get(alias_to_requested.get(m['tiger_id'], ''), m['tiger_id']),
                'matched_text': m.get('matched_alias')
            }
            for m in result.get('mentions', [])
        ]

        analyzed_comments.append({
            **comment,
            # author_name ãŒãªã„ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¯¾å¿œ
            'author_name': comment.get('author_name') or comment.get('author') or '',
            'normalized_text': result.get('normalized_text'),
            'tiger_mentions': mentions_for_ui  # ç©ºã§ã‚‚OK
        })

    # çµ±è¨ˆé›†è¨ˆ
    aggregator = StatsAggregator(tigers_file)
    stats = aggregator.calculate_video_stats(
        analyzed_comments=analyzed_comments,
        appearing_tigers=request.tiger_ids
    )

    # å‹•ç”»æƒ…å ±ã‚’å–å¾—ã—ã¦titleã‚’è¿½åŠ 
    videos_file = os.path.join(os.path.dirname(__file__), "../../../data/videos.json")
    video_title = "Unknown"
    if os.path.exists(videos_file):
        with open(videos_file, 'r', encoding='utf-8') as f:
            videos = json.load(f)
            video = next((v for v in videos if v['video_id'] == request.video_id), None)
            if video:
                video_title = video.get('title', 'Unknown')

    # çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ï¼ˆãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ç”¨ã«å¤‰æ›ï¼‰
    save_stats = {
        'video_id': request.video_id,
        'title': video_title,
        'total_comments': stats['N_total'],
        'tiger_mention_comments': stats['N_entity'],
        'tiger_stats': [
            {
                'tiger_id': stat['tiger_id'],
                'display_name': stat['display_name'],
                'mention_count': stat['N_tiger'],
                'rate_total': stat['Rate_total'] / 100,  # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆã‚’å°æ•°ã«
                'rate_entity': stat['Rate_entity'] / 100,
                'rank': stat['rank']
            }
            for stat in stats['tiger_stats'].values()
        ]
    }

    stats_file = os.path.join(
        os.path.dirname(__file__),
        f"../../../data/video_stats_{request.video_id}.json"
    )
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump(save_stats, f, ensure_ascii=False, indent=2)

    # åˆ†ææ¸ˆã¿ã‚³ãƒ¡ãƒ³ãƒˆã‚‚ä¿å­˜ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆä¸€è¦§è¡¨ç¤ºç”¨ï¼‰
    analyzed_comments_file = os.path.join(
        os.path.dirname(__file__),
        f"../../../data/analyzed_comments_{request.video_id}.json"
    )
    with open(analyzed_comments_file, 'w', encoding='utf-8') as f:
        json.dump(analyzed_comments, f, ensure_ascii=False, indent=2)

    # ========== DBæ°¸ç¶šåŒ– ==========
    db_warning = None  # DBæ°¸ç¶šåŒ–ã®è­¦å‘Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    try:
        # Video ãƒ¬ã‚³ãƒ¼ãƒ‰ã®å­˜åœ¨ç¢ºèª/ä½œæˆ
        video_in_db = db.query(VideoDB).filter(VideoDB.video_id == request.video_id).first()
        if not video_in_db:
            # videos.json ã‹ã‚‰è£œå®Œ
            videos_file = os.path.join(os.path.dirname(__file__), "../../../data/videos.json")
            video_meta = None
            if os.path.exists(videos_file):
                with open(videos_file, 'r', encoding='utf-8') as f:
                    vids = json.load(f)
                    video_meta = next((v for v in vids if v['video_id'] == request.video_id), None)
            video_in_db = VideoDB(
                video_id=request.video_id,
                title=(video_meta or {}).get('title', video_title),
                description=(video_meta or {}).get('description', ''),
                channel_id=(video_meta or {}).get('channel_id', ''),
                channel_title=(video_meta or {}).get('channel_title', ''),
                published_at=datetime.fromisoformat((video_meta or {}).get('published_at', '1970-01-01T00:00:00+00:00').replace('Z', '+00:00')) if (video_meta and video_meta.get('published_at')) else None,
                view_count=(video_meta or {}).get('view_count', 0),
                like_count=(video_meta or {}).get('like_count', 0),
                comment_count=len(comments),  # å®Ÿéš›ã«å–å¾—ã—ãŸã‚³ãƒ¡ãƒ³ãƒˆæ•°ã‚’ä½¿ç”¨
                thumbnail_url=(video_meta or {}).get('thumbnail_url', '')
            )
            db.add(video_in_db)
            db.flush()
        else:
            # æ—¢å­˜ã®VideoãŒã‚ã‚‹å ´åˆã€ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã‚’å®Ÿéš›ã®æ•°ã§æ›´æ–°
            video_in_db.comment_count = len(comments)

        # ========== VideoTiger ç™»éŒ² ==========
        # æ—¢å­˜ã®VideoTigeré–¢ä¿‚ã‚’å‰Šé™¤
        db.query(VideoTiger).filter(VideoTiger.video_id == request.video_id).delete()

        # å‡ºæ¼”ç¤¾é•·ã‚’ç™»éŒ²ï¼ˆDBã«å­˜åœ¨ã™ã‚‹ç¤¾é•·ã®ã¿ï¼‰
        for order, tiger_id in enumerate(request.tiger_ids, start=1):
            # ç¤¾é•·ãŒDBã«å­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
            tiger_exists = db.query(TigerDB).filter(TigerDB.tiger_id == tiger_id).first()
            if tiger_exists:
                video_tiger = VideoTiger(
                    video_id=request.video_id,
                    tiger_id=tiger_id,
                    appearance_order=order
                )
                db.add(video_tiger)
            else:
                print(f"[analyze] Warning: Tiger {tiger_id} not found in DB, skipping VideoTiger registration")

        # ã‚³ãƒ¡ãƒ³ãƒˆã®ã‚¢ãƒƒãƒ—ã‚µãƒ¼ãƒˆã¨è¨€åŠé–¢ä¿‚ã®æ›´æ–°
        for c in analyzed_comments:
            # ã‚³ãƒ¡ãƒ³ãƒˆæœ¬ä½“
            model = db.query(CommentDB).filter(CommentDB.comment_id == c['comment_id']).first()
            published_at = None
            try:
                if c.get('published_at'):
                    published_at = datetime.fromisoformat(c['published_at'].replace('Z', '+00:00'))
            except Exception:
                published_at = None
            if not model:
                model = CommentDB(
                    comment_id=c['comment_id'],
                    video_id=request.video_id,
                    text_original=c.get('text', ''),
                    normalized_text=c.get('normalized_text'),
                    author_name=c.get('author_name') or c.get('author') or '',
                    author_channel_id=c.get('author_channel_id') or '',
                    like_count=c.get('like_count') or 0,
                    published_at=published_at,
                    is_reply=bool(c.get('is_reply')),
                    parent_comment_id=c.get('parent_id')
                )
                db.add(model)
            else:
                model.text_original = c.get('text', '')
                model.normalized_text = c.get('normalized_text')
                model.author_name = c.get('author_name') or c.get('author') or ''
                model.author_channel_id = c.get('author_channel_id') or ''
                model.like_count = c.get('like_count') or 0
                model.published_at = published_at
                model.is_reply = bool(c.get('is_reply'))
                model.parent_comment_id = c.get('parent_id')

            # æ—¢å­˜ã®é–¢ä¿‚ã‚’å‰Šé™¤ã—ã¦ã‹ã‚‰å†ç™»éŒ²
            db.query(CommentTigerRelation).filter(CommentTigerRelation.comment_id == c['comment_id']).delete()
            for m in c.get('tiger_mentions', []):
                tid = m.get('tiger_id') or m.get('tigerId')
                if not tid:
                    continue
                # ç¤¾é•·ãŒDBã«å­˜åœ¨ã™ã‚‹ã‹ç¢ºèªï¼ˆå¤–éƒ¨ã‚­ãƒ¼åˆ¶ç´„ã‚¨ãƒ©ãƒ¼é˜²æ­¢ï¼‰
                tiger_exists = db.query(TigerDB).filter(TigerDB.tiger_id == tid).first()
                if not tiger_exists:
                    print(f"[analyze] Warning: Tiger {tid} not found in DB, skipping CommentTigerRelation")
                    continue
                rel = CommentTigerRelation(
                    comment_id=c['comment_id'],
                    tiger_id=tid,
                    matched_alias=m.get('matched_alias') or m.get('matched_text'),
                    match_method='rule_based',
                    confidence_score=1.0
                )
                db.add(rel)

        # VideoTigerStats ã‚’æ›´æ–°
        N_total = stats['N_total']
        N_entity = stats['N_entity']
        # ã„ã£ãŸã‚“ã“ã®å‹•ç”»ã®çµ±è¨ˆã‚’å‰Šé™¤ã—ã¦ã‹ã‚‰å†ä½œæˆ
        db.query(VideoTigerStats).filter(VideoTigerStats.video_id == request.video_id).delete()
        # é †ä½ä»˜ä¸æ¸ˆã¿statsã‹ã‚‰ç”Ÿæˆ
        ss = list(stats['tiger_stats'].values())
        for s in ss:
            # ç¤¾é•·ãŒDBã«å­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
            tiger_exists = db.query(TigerDB).filter(TigerDB.tiger_id == s['tiger_id']).first()
            if not tiger_exists:
                print(f"[analyze] Warning: Tiger {s['tiger_id']} not found in DB, skipping VideoTigerStats")
                continue
            db.add(VideoTigerStats(
                video_id=request.video_id,
                tiger_id=s['tiger_id'],
                n_total=N_total,
                n_entity=N_entity,
                n_tiger=s['N_tiger'],
                rate_total=(s['Rate_total'] / 100.0 if s['Rate_total'] else 0.0),
                rate_entity=(s['Rate_entity'] / 100.0 if s['Rate_entity'] else 0.0),
                rank=s.get('rank')
            ))

        db.commit()
        print(f"[analyze] DB persistence successful for video {request.video_id}")
    except Exception as e:
        # DBã¸ã®æ°¸ç¶šåŒ–å¤±æ•—ï¼šãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¦è­¦å‘Šã‚’è¨˜éŒ²
        db.rollback()
        import traceback
        error_detail = f"{e}\n{traceback.format_exc()}"
        print(f"[analyze] DB persistence failed: {error_detail}")
        db_warning = f"DBæ°¸ç¶šåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}"

    processing_time = time.time() - start_time

    # è¨€åŠæ•°ã‚’é›†è¨ˆ
    tiger_mentions = {}
    for t in tigers:
        count = sum(
            1 for c in analyzed_comments
            if any(m['tiger_id'] == t['tiger_id'] for m in c['tiger_mentions'])
        )
        tiger_mentions[t['tiger_id']] = count

    return AnalysisResult(
        video_id=request.video_id,
        total_comments=len(comments),
        analyzed_comments=len([c for c in analyzed_comments if c['tiger_mentions']]),
        tiger_mentions=tiger_mentions,
        processing_time=processing_time
    )


@router.get("/comments/{video_id}")
async def get_analyzed_comments(video_id: str, tiger_id: str = None):
    """
    åˆ†ææ¸ˆã¿ã‚³ãƒ¡ãƒ³ãƒˆã‚’å–å¾—ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ç¤¾é•·IDã§ãƒ•ã‚£ãƒ«ã‚¿ï¼‰
    """
    analyzed_comments_file = os.path.join(
        os.path.dirname(__file__),
        f"../../../data/analyzed_comments_{video_id}.json"
    )

    if not os.path.exists(analyzed_comments_file):
        raise HTTPException(
            status_code=404,
            detail=f"Analyzed comments for video {video_id} not found. Please analyze first."
        )

    with open(analyzed_comments_file, 'r', encoding='utf-8') as f:
        analyzed_comments = json.load(f)

    # ç¤¾é•·IDã§ãƒ•ã‚£ãƒ«ã‚¿
    if tiger_id:
        filtered_comments = [
            c for c in analyzed_comments
            if any(m['tiger_id'] == tiger_id for m in c.get('tiger_mentions', []))
        ]
        return filtered_comments

    # è¨€åŠãŒã‚ã‚‹ã‚³ãƒ¡ãƒ³ãƒˆã®ã¿è¿”ã™
    return [c for c in analyzed_comments if c.get('tiger_mentions')]


@router.get("/video-tigers/{video_id}")
async def get_video_tigers(video_id: str, db: Session = Depends(get_db)):
    """
    å‹•ç”»ã«ç™»éŒ²æ¸ˆã¿ã®ç¤¾é•·ä¸€è¦§ã‚’å–å¾—
    """
    video_tigers = db.query(VideoTiger).filter(VideoTiger.video_id == video_id).order_by(VideoTiger.appearance_order).all()

    if not video_tigers:
        return {"video_id": video_id, "tigers": [], "has_registered": False}

    # ç¤¾é•·æƒ…å ±ã‚’å–å¾—
    tiger_ids = [vt.tiger_id for vt in video_tigers]
    tigers_db = db.query(TigerDB).filter(TigerDB.tiger_id.in_(tiger_ids)).all()
    tiger_map = {t.tiger_id: t for t in tigers_db}

    tigers = []
    for vt in video_tigers:
        tiger = tiger_map.get(vt.tiger_id)
        if tiger:
            tigers.append({
                "tiger_id": tiger.tiger_id,
                "display_name": tiger.display_name,
                "full_name": tiger.full_name,
                "image_url": tiger.image_url,
                "appearance_order": vt.appearance_order
            })

    return {"video_id": video_id, "tigers": tigers, "has_registered": True}
